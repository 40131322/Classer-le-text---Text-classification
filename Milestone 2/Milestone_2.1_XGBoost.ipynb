{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f238db48-b45f-4285-a9fd-af78cb43f052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\anaconda3\\envs\\tf_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2ceb41-e2aa-46b6-ad0b-58e3ffd9f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE,SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f4052a-924d-42f4-92fb-d49b1d0f90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data loaded.\n",
      "Loading training labels...\n",
      "Training labels loaded.\n",
      "Loading test data...\n",
      "Test data loaded.\n",
      "Loading vocabulary...\n",
      "Vocab data loaded.\n",
      "Training data shape: (9422, 26354)\n",
      "Training labels shape: (9422,)\n",
      "Test data shape: (2356, 26354)\n",
      "Vocab data shape: (26354,)\n"
     ]
    }
   ],
   "source": [
    "# Load and display data\n",
    "print(\"Loading training data...\")\n",
    "X_train = np.load(\"D:/GitHub/Classer-le-text---Text-classification/Data/data_train.npy\").astype(int)\n",
    "print(\"Training data loaded.\")\n",
    "\n",
    "print(\"Loading training labels...\")\n",
    "y_train_raw = np.loadtxt('D:/GitHub/Classer-le-text---Text-classification/Data/label_train.csv', skiprows=1, delimiter=',').astype(int)[:, 1]\n",
    "print(\"Training labels loaded.\")\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "X_test = np.load(\"D:/GitHub/Classer-le-text---Text-classification/Data/data_test.npy\").astype(int)\n",
    "print(\"Test data loaded.\")\n",
    "\n",
    "print(\"Loading vocabulary...\")\n",
    "vocab_data = np.load(\"D:/GitHub/Classer-le-text---Text-classification/Data/vocab_map.npy\", allow_pickle=True)\n",
    "print(\"Vocab data loaded.\")\n",
    "\n",
    "# Check dimensions\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train_raw.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Vocab data shape: {vocab_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6665cad-274f-4cd5-9dc9-2738e3e1c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Log transformation to stabilize variance\n",
    "X_train_log = np.log1p(X_train)\n",
    "X_test_log = np.log1p(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf00c874-2ba4-4c18-b26f-2a8ce0ac5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stop word removal\n",
    "stop_words = set([\n",
    "    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'while', 'with', 'without', 'of', 'at', 'by', 'for', 'to', 'in', 'on',\n",
    "    'from', 'up', 'down', 'out', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "    'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'don', 'should', 'now','about', 'being', 'thereby',    'aiming', 'didn', 'deciding', 'derive', 'foretell', 'concede', 'prepare',\n",
    "    'behind', 'withstand', 'upper', 'further', 'alreadyoverwhelming', 'minimal'\n",
    "])\n",
    "\n",
    "# Create a mask for non-stop-word features\n",
    "stop_word_indices = [i for i, word in enumerate(vocab_data) if word in stop_words]\n",
    "mask = np.ones(len(vocab_data), dtype=bool)\n",
    "mask[stop_word_indices] = False\n",
    "\n",
    "# Apply the mask to exclude stop words from X_train and X_test\n",
    "X_train_filtered = X_train_log[:, mask]\n",
    "X_test_filtered = X_test_log[:, mask]\n",
    "filtered_vocab = [word for i, word in enumerate(vocab_data) if mask[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f308499-f317-4adb-b161-9665287aa9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcdc49a8-65cc-41b0-a492-5b2cce81bf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected vocabulary size after ANOVA: 500\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply ANOVA to select the top N features after stop word removal\n",
    "num_features_anova = 500  # Adjust based on experimentation\n",
    "anova_selector = SelectKBest(score_func=f_classif, k=num_features_anova)\n",
    "X_train_anova = anova_selector.fit_transform(X_train_filtered, y_train_raw)\n",
    "X_test_anova = anova_selector.transform(X_test_filtered)\n",
    "\n",
    "# Retrieve selected feature indices from ANOVA selection\n",
    "anova_selected_indices = anova_selector.get_support(indices=True)\n",
    "selected_vocab_anova = [filtered_vocab[i] for i in anova_selected_indices]\n",
    "\n",
    "print(f\"Selected vocabulary size after ANOVA: {len(selected_vocab_anova)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bd5919-4e7c-4903-bbd9-267a5020eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected words after ANOVA selection:\n",
      "['abstractive', 'accelerated', 'account', 'achieve', 'achieves', 'acoustic', 'agent', 'agents', 'agnostic', 'ai', 'al', 'algorithm', 'alternating', 'application', 'approximate', 'approximation', 'arabic', 'are', 'area', 'areas', 'armed', 'arms', 'art', 'article', 'artificial', 'asr', 'assessment', 'assumption', 'assumptions', 'asymptotic', 'automated', 'automatic', 'autonomous', 'autoregressive', 'available', 'bandit', 'bandits', 'based', 'baselines', 'basic', 'batch', 'be', 'been', 'behaved', 'belief', 'bleu', 'bound', 'bounds', 'build', 'calculus', 'careful', 'carlo', 'categories', 'category', 'characteristics', 'cifar', 'class', 'classify', 'cognitive', 'compare', 'competitive', 'completion', 'compositional', 'compositionality', 'comprehensive', 'computationally', 'computer', 'concept', 'concepts', 'conditioned', 'conducted', 'conll', 'consider', 'considered', 'constant', 'constituency', 'content', 'contrast', 'convergence', 'convergent', 'convex', 'convexity', 'convolution', 'copying', 'corruptions', 'could', 'covariance', 'database', 'databases', 'datasets', 'decision', 'decoder', 'decoding', 'demonstrate', 'dependency', 'descent', 'describe', 'described', 'describes', 'description', 'detect', 'detection', 'develop', 'developed', 'development', 'diagnosis', 'different', 'differentiable', 'dimension', 'dimensional', 'discovery', 'discrepancy', 'discrete', 'discriminative', 'discuss', 'discussed', 'discusses', 'distribution', 'distributions', 'divergence', 'downstream', 'dp', 'dpp', 'draws', 'dropout', 'duality', 'dynamic', 'efficient', 'eigenvectors', 'ell_1', 'embeddings', 'emerging', 'empirical', 'empirically', 'employed', 'encoder', 'encoders', 'encourage', 'encourages', 'engineered', 'enjoys', 'entries', 'environment', 'epsilon', 'establish', 'estimates', 'estimating', 'estimation', 'estimator', 'estimators', 'et', 'etc', 'evaluated', 'evolutionary', 'examined', 'existing', 'experiments', 'experts', 'exponential', 'exponentially', 'extended', 'extracted', 'extracting', 'family', 'faster', 'field', 'focus', 'formal', 'found', 'frac', 'frank', 'function', 'functions', 'fusion', 'future', 'fuzzy', 'gains', 'gamma', 'gaussian', 'generalization', 'generalize', 'generative', 'genetic', 'german', 'gibbs', 'globally', 'gp', 'gradient', 'gradients', 'greedy', 'guarantees', 'hastings', 'have', 'help', 'heuristic', 'hidden', 'hilbert', 'huge', 'human', 'hyperparameters', 'identifiability', 'identification', 'identify', 'imagenet', 'implemented', 'improvements', 'improves', 'increasing', 'inducing', 'inequalities', 'inference', 'information', 'intelligence', 'intelligent', 'interaction', 'internet', 'into', 'intractable', 'introduce', 'investigated', 'is', 'ising', 'issue', 'iterate', 'iterations', 'joint', 'jointly', 'kernel', 'knowledge', 'langevin', 'large', 'lasso', 'last', 'latent', 'layers', 'learn', 'learning', 'learns', 'lets', 'likelihood', 'linear', 'linearly', 'literature', 'log', 'logarithmic', 'logic', 'logics', 'loss', 'low', 'lower', 'lstm', 'lstms', 'magnitude', 'management', 'margin', 'marginal', 'markedly', 'matches', 'matching', 'mathcal', 'matrices', 'matrix', 'maximization', 'maximizing', 'may', 'mcmc', 'means', 'methods', 'mild', 'minimax', 'minimization', 'mining', 'mixing', 'mobile', 'model', 'models', 'monitoring', 'monte', 'namely', 'need', 'neural', 'nmt', 'noise', 'non', 'nonconvex', 'nonparametric', 'nonsmooth', 'norm', 'notoriously', 'novel', 'nowadays', 'nuclear', 'objective', 'obtained', 'omega', 'ontologies', 'ontology', 'optimism', 'optimization', 'optimize', 'oracle', 'orthogonal', 'our', 'outperforms', 'overview', 'pairs', 'paper', 'parameter', 'parameters', 'parametric', 'parser', 'parsers', 'parsing', 'part', 'pattern', 'patterns', 'penalized', 'penn', 'people', 'performance', 'performed', 'perturbed', 'plan', 'planning', 'platform', 'platforms', 'poisson', 'polytope', 'possibility', 'posterior', 'practice', 'prediction', 'preliminary', 'presented', 'presents', 'previous', 'prior', 'process', 'processing', 'programming', 'project', 'propose', 'proposed', 'provably', 'prove', 'proximal', 'purpose', 'quadratic', 'random', 'randomized', 'rank', 'rates', 'razor', 'reasoning', 'recently', 'recognition', 'recover', 'recovers', 'recovery', 'recurrent', 'regression', 'regret', 'regularization', 'regularizations', 'regularized', 'regularizer', 'regularizers', 'relaxation', 'relaxed', 'reliable', 'representations', 'representing', 'reproducing', 'require', 'reranking', 'rescaling', 'research', 'review', 'reward', 'rich', 'ridge', 'robot', 'rules', 'sag', 'saga', 'sample', 'sampled', 'sampler', 'samplers', 'sampling', 'scalable', 'scales', 'science', 'sdca', 'search', 'security', 'semantics', 'sensor', 'sentence', 'sequence', 'services', 'set', 'setting', 'settings', 'show', 'showed', 'showing', 'significantly', 'simple', 'simpler', 'simulation', 'singular', 'small', 'smart', 'smooth', 'smoothness', 'social', 'softmax', 'software', 'solutions', 'sources', 'sparse', 'sparsity', 'specification', 'spectral', 'speech', 'sqrt', 'squared', 'squares', 'standard', 'state', 'stochastic', 'strong', 'strongly', 'structured', 'subgradient', 'sublinear', 'submodular', 'substantial', 'substantially', 'supervised', 'supervision', 'support', 'survey', 'svrg', 'synthetic', 'system', 'systems', 'taken', 'task', 'tasks', 'techniques', 'technologies', 'technology', 'temporal', 'tensor', 'tested', 'that', 'their', 'theoretical', 'theory', 'this', 'those', 'three', 'tilde', 'tool', 'tools', 'tractable', 'traffic', 'train', 'trained', 'training', 'transition', 'translation', 'treebank', 'try', 'ucb', 'unbiased', 'unbounded', 'uncertainty', 'unknown', 'unlabeled', 'used', 'user', 'users', 'using', 'utilized', 'variance', 'variational', 'various', 'vectors', 'verification', 'was', 'we', 'web', 'were', 'wmt', 'wolfe', 'word', 'worst']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Selected words after ANOVA selection:\")\n",
    "print(selected_vocab_anova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5286f52-dfdc-4dce-9260-669fe501b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled training data shape: (7983, 500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Handle class imbalance with SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train_anova, y_train_raw)\n",
    "print(f\"Resampled training data shape: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c157676-15a4-4737-9d45-917df597b55b",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf17ea72-9ee5-4fd5-b6bb-03f1c0b1707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Feature Importance with XGBoost on resampled data\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get feature importances from XGBoost model and select top features\n",
    "importances = xgb_model.feature_importances_\n",
    "top_n_features_xgb = 30  # Adjust based on experimentation\n",
    "top_n_indices_xgb = np.argsort(importances)[-top_n_features_xgb:]\n",
    "\n",
    "# Map back to the original feature indices after ANOVA\n",
    "final_selected_indices = [anova_selected_indices[i] for i in top_n_indices_xgb]\n",
    "final_vocabulary = [filtered_vocab[i] for i in final_selected_indices]\n",
    "\n",
    "# Step 7: Final feature selection for training and testing\n",
    "X_train_final = X_train_resampled[:, top_n_indices_xgb]\n",
    "X_test_final = X_test_anova[:, top_n_indices_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c9fbbb-a30d-4c08-82be-8d6de692b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['may', 'three', 'bounds', 'our', 'gradient', 'information', 'sparse', 'convex', 'presented', 'systems', 'mining', 'decision', 'singular', 'paper', 'research', 'was', 'techniques', 'intelligence', 'belief', 'learning', 'system', 'theoretical', 'agents', 'achieves', 'development', 'used', 'art', 'we', 'logic', 'discuss']\n"
     ]
    }
   ],
   "source": [
    "print(final_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0916421-412c-49ed-9a99-502329961cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_final: (7983, 30)\n",
      "Shape of X_test_final: (2356, 30)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of the final selected training and test datasets\n",
    "print(\"Shape of X_train_final:\", X_train_final.shape)\n",
    "print(\"Shape of X_test_final:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf89b6bd-c135-40d0-bcb8-789916c8156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight based on resampled data\n",
    "num_negative = np.sum(y_train_resampled == 0)\n",
    "num_positive = np.sum(y_train_resampled == 1)\n",
    "scale_pos_weight = num_negative / num_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a91bcdb-67fb-4cff-b190-569ded540ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'scale_pos_weight': [1, 5, 10]  # Adjust based on class imbalance\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85139f9-594c-4a46-bbb9-d25903751653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 0.8, 'scale_pos_weight': 1, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 0.5, 'colsample_bytree': 0.6}\n",
      "Best F1 Score: 0.9624798082526336\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    scoring='f1',  # Scoring metric to optimize\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search.fit(X_train_final, y_train_resampled)\n",
    "\n",
    "# Output the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best F1 Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773f2e23-f300-4918-bf32-a02388d43091",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='logloss', \n",
    "    scale_pos_weight=1,  # Best parameter from tuning\n",
    "    subsample=0.8,       # Best parameter from tuning\n",
    "    n_estimators=50,     # Best parameter from tuning\n",
    "    min_child_weight=1,  # Best parameter from tuning\n",
    "    max_depth=7,         # Best parameter from tuning\n",
    "    learning_rate=0.1,   # Best parameter from tuning\n",
    "    gamma=0.5,           # Best parameter from tuning\n",
    "    colsample_bytree=0.6, # Best parameter from tuning\n",
    "    random_state=42,\n",
    "    reg_alpha=0.1,       # Regularization parameters to control overfitting\n",
    "    reg_lambda=1.0       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9aa50c-6c37-4d35-8c4b-174bc9da4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cross-validation metrics storage\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_precision = []\n",
    "cross_val_recall = []\n",
    "cross_val_f1 = []\n",
    "conf_matrices = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8553aafd-2e99-4d96-89ae-a53caaf0bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Precision: 0.9545\n",
      "Cross-Validation Recall: 0.9812\n",
      "Cross-Validation F1 Score: 0.9676\n",
      "Confusion Matrix for Fold 1:\n",
      "[[ 195   64]\n",
      " [  28 1310]]\n",
      "\n",
      "Confusion Matrix for Fold 2:\n",
      "[[ 198   61]\n",
      " [  29 1309]]\n",
      "\n",
      "Confusion Matrix for Fold 3:\n",
      "[[ 189   70]\n",
      " [  25 1313]]\n",
      "\n",
      "Confusion Matrix for Fold 4:\n",
      "[[ 197   62]\n",
      " [  21 1316]]\n",
      "\n",
      "Confusion Matrix for Fold 5:\n",
      "[[ 202   56]\n",
      " [  23 1315]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over each fold of resampled data\n",
    "for train_index, val_index in kf.split(X_train_final, y_train_resampled):\n",
    "    # Split resampled data into training and validation for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_final[train_index], X_train_final[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    final_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = final_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate metrics for the validation fold\n",
    "    precision = precision_score(y_val_fold, y_val_pred, zero_division=0)\n",
    "    recall = recall_score(y_val_fold, y_val_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, zero_division=0)\n",
    "    \n",
    "    # Store metrics for averaging later\n",
    "    cross_val_precision.append(precision)\n",
    "    cross_val_recall.append(recall)\n",
    "    cross_val_f1.append(f1)\n",
    "    \n",
    "    # Store confusion matrix for the validation fold\n",
    "    conf_matrices.append(confusion_matrix(y_val_fold, y_val_pred))\n",
    "\n",
    "# Calculate and display average cross-validation scores\n",
    "avg_precision = np.mean(cross_val_precision)\n",
    "avg_recall = np.mean(cross_val_recall)\n",
    "avg_f1 = np.mean(cross_val_f1)\n",
    "\n",
    "print(f\"Cross-Validation Precision: {avg_precision:.4f}\")\n",
    "print(f\"Cross-Validation Recall: {avg_recall:.4f}\")\n",
    "print(f\"Cross-Validation F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "# Optional: display confusion matrices for each fold\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    print(f\"Confusion Matrix for Fold {i + 1}:\\n{cm}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f038fc3f-b9a6-46bc-8cfa-dfc8e77d4b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test data:\n",
      "[1 0 1 ... 0 1 1]\n",
      "[1 0 1 1 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "print(\"Predictions on test data:\")\n",
    "print(y_test_pred)\n",
    "print(y_test_pred[:10])  # Print the first 10 predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1737ed29-fd4e-4c7e-b221-69a33b924be2",
   "metadata": {},
   "source": [
    "# Define range of features to test (0 to 100 in increments of 5)\n",
    "feature_counts = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 30, 50]\n",
    "\n",
    "\n",
    "# Initialize storage for results\n",
    "results = []\n",
    "\n",
    "# Loop over different numbers of final features\n",
    "for num_features_anova in feature_counts:\n",
    "    print(f\"Running with {num_features_anova} ANOVA-selected features...\")\n",
    "\n",
    "    top_n_features_xgb = num_features_anova  # Adjust based on experimentation\n",
    "    top_n_indices_xgb = np.argsort(importances)[-top_n_features_xgb:]\n",
    "    \n",
    "    # Map back to the original feature indices after ANOVA\n",
    "    final_selected_indices = [anova_selected_indices[i] for i in top_n_indices_xgb]\n",
    "    final_vocabulary = [filtered_vocab[i] for i in final_selected_indices]\n",
    "    \n",
    "    # Step 7: Final feature selection for training and testing\n",
    "    X_train_final = X_train_resampled[:, top_n_indices_xgb]\n",
    "    X_test_final = X_test_anova[:, top_n_indices_xgb]\n",
    "        \n",
    "    # Handle class imbalance with SMOTEENN\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train_anova, y_train_raw)\n",
    "\n",
    "    # Fit XGBoost model to select important features\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
    "    xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Get feature importances and select top features\n",
    "    importances = xgb_model.feature_importances_\n",
    "    top_n_features_xgb = min(num_features_anova, len(importances))  # Limit to available features\n",
    "    top_n_indices_xgb = np.argsort(importances)[-top_n_features_xgb:]\n",
    "\n",
    "    # Final selected features for training/testing\n",
    "    X_train_final = X_train_resampled[:, top_n_indices_xgb]\n",
    "    X_test_final = X_test_anova[:, top_n_indices_xgb]\n",
    "\n",
    "    # Calculate scale_pos_weight based on resampled data\n",
    "    num_negative = np.sum(y_train_resampled == 0)\n",
    "    num_positive = np.sum(y_train_resampled == 1)\n",
    "    scale_pos_weight = num_negative / num_positive\n",
    "\n",
    "    # Initialize and train the model for this feature count\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cross_val_f1_scores = cross_val_score(final_model, X_train_final, y_train_resampled, cv=kf, scoring='f1')\n",
    "\n",
    "    # Store results\n",
    "    avg_f1 = np.mean(cross_val_f1_scores)\n",
    "    print(f\"Average F1 Score for {num_features_anova} features: {avg_f1:.4f}\")\n",
    "    results.append((num_features_anova, avg_f1))\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"Feature Selection Results:\")\n",
    "for num_features, f1 in results:\n",
    "    print(f\"{num_features} features: F1 Score = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c3133-0d42-4330-a686-697bcc2f29d7",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52012d74-83f3-4d52-aa86-6734a1cf7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of features to test\n",
    "feature_counts = [50, 150,  250, 350,  450]\n",
    "\n",
    "# Parameters for the Random Forest model\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "# Loop over each feature count\n",
    "for num_features_anova in feature_counts:\n",
    "    print(f\"\\nTesting with {num_features_anova} ANOVA-selected features...\")\n",
    "\n",
    "    # Step 1: Apply ANOVA feature selection\n",
    "    anova_selector = SelectKBest(score_func=f_classif, k=num_features_anova)\n",
    "    X_train_anova = anova_selector.fit_transform(X_train_filtered, y_train_raw)\n",
    "    X_test_anova = anova_selector.transform(X_test_filtered)\n",
    "\n",
    "    # Retrieve selected feature indices for reference if needed\n",
    "    anova_selected_indices = anova_selector.get_support(indices=True)\n",
    "    selected_vocab_anova = [filtered_vocab[i] for i in anova_selected_indices]\n",
    "\n",
    "    # Step 2: Initialize the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Step 3: Cross-validation with Grid Search\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1',\n",
    "        cv=kf,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Fit the grid search on the ANOVA-selected features\n",
    "    print(\"Running grid search for optimal hyperparameters...\")\n",
    "    grid_search.fit(X_train_anova, y_train_raw)\n",
    "\n",
    "    # Get the best hyperparameters and F1 score for this feature count\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_score = grid_search.best_score_\n",
    "    print(f\"Best F1 Score with {num_features_anova} features: {best_f1_score:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append((num_features_anova, best_params, best_f1_score))\n",
    "\n",
    "# Display results for each feature count\n",
    "print(\"\\nFeature Selection and Hyperparameter Tuning Results:\")\n",
    "for num_features, best_params, f1_score in results:\n",
    "    print(f\"{num_features} features: Best Params = {best_params}, F1 Score = {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ec5046-56e3-42e7-a407-1f784fde5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array_with_index(array, filename):\n",
    "    # Create a DataFrame with the array\n",
    "    df = pd.DataFrame(array, columns=['label'])\n",
    "    \n",
    "    # Add the 'id' column using the row index\n",
    "    df['ID'] = df.index\n",
    "    \n",
    "    # Reorder the columns to have 'id' first and 'label' second\n",
    "    df = df[['ID', 'label']]\n",
    "    \n",
    "    # Check if the file exists and save over it\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File {filename} exists. Saving over it.\")\n",
    "    else:\n",
    "        print(f\"File {filename} does not exist. Creating a new file.\")\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d305c210-99be-4fbe-b278-0c9bc5baf362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File prediction_milestone2_XGBoost.csv exists. Saving over it.\n"
     ]
    }
   ],
   "source": [
    "filename = 'prediction_milestone2_XGBoost.csv'\n",
    "\n",
    "save_array_with_index(y_test_pred, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bba1ef-b297-4262-9a18-9bc11e3ba422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
